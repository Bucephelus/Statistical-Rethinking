[[Statistical_Rethinking__A_Bayesian_Course_With_Examples_in_R_and_STAN--McE.pdf]]

[[Statistical Rethinking]]

#statsbook #bayesian 

*"Animated by the truth, but lacking free will, a golem always does exactly what it is told."* (1)
### 1.1 Statistical Golems 
- Stat models can be like *golems* in the sense that they are concerned with modelling the world, but there have 0 morality on their own. 
- Need to separate the mathematical procedure of the stats modelling from the interpretation or implication it has.
- Stats is seen as an objective, a deterministic check list of doing science
- McElreath compares this to plumbers doing their jobs without knowing fluid dynamics. If the user steps outside of the intended use by extrapolating - imagine having hydraulic engineers by promoting plumbers. 
#### 1.1.1 Good to use any model 
- McElreath hints that classical statistical methods are *inflexible* and *fragile* yet used so frequently, are rarely truly appropriate.
- He says that classical tools are not diverse enough to handle many common research questions. 
### 1.2 Statistical Rethinking
- Deductive falsification is impossible in *nearly* every scientific context. 
	- Hypotheses are not models - the relations among hypotheses and different kinds of models are complex. Strict falsification is impossible where many models can correspond to the same hypothesis, and many hypotheses correspond to a single model. 
	- **Measurement** matters - even if the data falsifies a model, another observer will debate methods and measures and they *won't* trust the data. Maybe they are right...
- Thus deductive falsification will never work, the scientific method cannot be simply reduced to a statistical procedure, and so our statistical methods should not pretend.
#### 1.2.1 Hypotheses are not models 
- All models are *technically* false, so what does it actually mean to **falsify** a model? 
- Process models: more specific possibilities of actual data generating processes - conceptual, time dependent, causal structures of what *could* explain a phenomenon 
- Statistical models: translate the above causal models into an associative statistical model to be able to estimate things. These models can be a result of multiple process models, even from different hypotheses. 
- The statistical model is what is represented by the statistical procedure, but that certain representation can be the same with different data-generating processes since the data can be generated by completely different processes, even from other hypotheses. This is the problem with hypotheses and models. 

### 1.4 Intro to Bayes 
- Many states of the world could very much produce some sort of observation but that doesn't necessarily mean that state of the world is therefore likely to be the one that did. This is related to the *fallacy of the transposed conditional* : given two events *A* and *B*, the probability of *A*, happening given that *B* has happened is assumed to be about the same as the probability of *B* given *A*, when there is actually no evidence for such an assumption. Also known as the **"Confusion of the inverse".
- “But falsification is always consensual, not logical. In light of real problems of measurement error and the continuous nature of natural phenomena, scientific communities argue towards consensus about the meaning of evidence.” (9)
- We cannot objectively, and logically falsify something completely. 
- *Modus tollens* - all swans that any European had ever seen had white feathers. This led to the belief that *all* swans are white. The formal hypothesis was that. Once they reached Australia, they observed one swan with black feathers. Alas, this evidence instantly disproved their hypothesis. Indeed, not all swans are white. Thus, the insight here is that before venturing to Australia, no number of observations (their data) of the white swans could prove their hypothesis false, and it only required only 1 observation of a black swan to prove it false. 
- We can thus improve our statistical model - *always* look for evidence that disconfirms our hypotheses: *whenever we find a black swan, $H_0$ must be false*. 
### 1.5 Tools for golem engineering
- We want to use our models for: designing inquiry, extracting info from data and making predictions. The tools used will be: 
	- Bayesian data analysis 
	- Model comparison 
	- Multilevel models
	- Graphical causal models
- Bayesian data analysis - no more than counting the number of ways the data we have could happen, according to our assumptions. Things that can happen more ways are more plausible. Once we have defined our statistical model, Bayesian D.A forces a logical way of processing our data to produce *inference*. 
- *From the perspective of our golem, the coin toss is "random", but it's really the golem that is random, not the coin*. 
- Bayesian D.A provides a way for models to learn from data, but when there is more than one plausible model, how should we choose among them? Just...prefer models that make good predictions. We want to compare models based upon expected predictive accuracy, *cross-validation* and *information criteria*. 
- Complex models often make worse predictions that simpler models due to *over-fitting*. Future data will not be exactly like pasta data, and so any model that is unaware of such a fact will tend to make worse predictions than it could. 
- *The smarter the golem, the dumber its predictions.*
- *Fitting is easy; prediction is hard*. 
#### 1.3.3 Multilevel models 
- Statistics models contain parameters. Sometimes in these models, it is parameters all the way down. Any particular parameter can be regarded as a placeholder for a missing model. This leads to a model with multiple levels of uncertainty, each feeding into the next - a multilevel model.
- Multilevel models help us deal with over-fitting. Cross-validation and information criteria measure our overfitting risk and help us to recognise it. M.Ms actually do something about it. They exploit partial pooling that pools information across units in the data, in order to produces better estimates for all units. 
- Adjust estimates for repeat sampling, adjust estimates for imbalance in sampling, study variation, and avoid averaging.
#### 1.3.4 Graphical Causal Models 
- When the wind blows, branches sway. As a human, we interpret this statement as causal: the wind makes the branches move. In reality, this is a statistical association. From the data lone, it could also be that the branches swaying makes the wind. 
- *A statistical model is never sufficient for inferring cause, because the statistical model makes no distinction between the wind causing the branches to sway and the branches causing the wind to blow.*
- Facts outside the data are needed to decide which explanation is the correct one. 
- Successful prediction does not require correct causal identification.
- Directed acylic graphs (DAGs) are heuristic but they allow us to deduce which statistical models can provide valid causal inferences, assuming the DAG is true. 
- There will never be a golem that accepts naked data and returns a reliable model of the causal relations among the variables. 